{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55aa31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CXR Report Evaluation - Modular Architecture\n",
    "# Updated to use the new modular metrics system\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import pickle\n",
    "import hashlib\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Import the new modular metrics\n",
    "from CXRMetric.metrics.rouge import ROUGEEvaluator\n",
    "from CXRMetric.metrics.bleu import BLEUEvaluator\n",
    "from CXRMetric.metrics.bertscore import BERTScoreEvaluator\n",
    "from CXRMetric.metrics.perplexity import PerplexityEvaluator\n",
    "from CXRMetric.metrics.composite import CompositeEvaluator\n",
    "from CXRMetric.metrics.semantic_embedding import SemanticEmbeddingEvaluator\n",
    "\n",
    "# Import configuration\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from config import *\n",
    "\n",
    "# Set up plotting style\n",
    "sns.set(style='whitegrid')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üöÄ CXR Report Evaluation - Modular Architecture\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Available metrics:\")\n",
    "print(\"  ‚úÖ ROUGE-L (lightweight)\")\n",
    "print(\"  ‚úÖ BLEU-4 (lightweight)\")\n",
    "print(\"  ‚úÖ BERTScore (moderate GPU usage)\")\n",
    "print(\"  ‚úÖ Perplexity (GPU-accelerated)\")\n",
    "print(\"  ‚úÖ Composite RadCliQ (v0, v1)\")\n",
    "print(\"  ‚úÖ Semantic Embedding (CheXbert)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Initialize metric evaluators (will be done later with user options)\n",
    "evaluators = {}\n",
    "\n",
    "def initialize_evaluators(config_options):\n",
    "    \"\"\"Initialize metric evaluators based on user configuration.\"\"\"\n",
    "    global evaluators\n",
    "    evaluators = {}\n",
    "    \n",
    "    # Always available lightweight metrics\n",
    "    evaluators['rouge'] = ROUGEEvaluator(beta=1.2)\n",
    "    evaluators['bleu'] = BLEUEvaluator()\n",
    "    \n",
    "    # GPU-dependent metrics\n",
    "    if config_options.get('enable_bertscore', True):\n",
    "        evaluators['bertscore'] = BERTScoreEvaluator(use_idf=config_options.get('use_idf', False))\n",
    "    \n",
    "    if config_options.get('enable_perplexity', False):\n",
    "        model_name = config_options.get('perplexity_model', 'distilgpt2')\n",
    "        evaluators['perplexity'] = PerplexityEvaluator(model_name=model_name)\n",
    "    \n",
    "    if config_options.get('enable_composite', True):\n",
    "        evaluators['composite'] = CompositeEvaluator()\n",
    "    \n",
    "    if config_options.get('enable_semantic', False):\n",
    "        evaluators['semantic'] = SemanticEmbeddingEvaluator()\n",
    "    \n",
    "    print(f\"üìä Initialized {len(evaluators)} metric evaluators\")\n",
    "    return evaluators\n",
    "\n",
    "def get_cache_key(input_paths, options):\n",
    "    \"\"\"Generate a unique cache key based on file paths and evaluation options.\"\"\"\n",
    "    file_info = []\n",
    "    for path in input_paths:\n",
    "        if os.path.exists(path):\n",
    "            mtime = os.path.getmtime(path)\n",
    "            file_info.append(f\"{path}:{mtime}\")\n",
    "    \n",
    "    content = \"|\".join(file_info) + \"|\" + str(sorted(options.items()))\n",
    "    return hashlib.md5(content.encode()).hexdigest()\n",
    "\n",
    "def load_cached_result(cache_key, cache_dir=\"cache/results\"):\n",
    "    \"\"\"Load cached evaluation result if it exists.\"\"\"\n",
    "    cache_path = os.path.join(cache_dir, f\"{cache_key}.pkl\")\n",
    "    if os.path.exists(cache_path):\n",
    "        try:\n",
    "            with open(cache_path, 'rb') as f:\n",
    "                cached_data = pickle.load(f)\n",
    "            print(f\"‚úì Loaded cached result from {cache_path}\")\n",
    "            return cached_data['pred_df'], cached_data['summary']\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load cache: {e}\")\n",
    "    return None\n",
    "\n",
    "def save_cached_result(cache_key, pred_df, summary, cache_dir=\"cache/results\"):\n",
    "    \"\"\"Save evaluation result to cache for future runs.\"\"\"\n",
    "    os.makedirs(cache_dir, exist_ok=True)\n",
    "    cache_path = os.path.join(cache_key, f\"{cache_key}.pkl\")\n",
    "    try:\n",
    "        cached_data = {\n",
    "            'pred_df': pred_df,\n",
    "            'summary': summary,\n",
    "            'timestamp': time.time()\n",
    "        }\n",
    "        with open(cache_path, 'wb') as f:\n",
    "            pickle.dump(cached_data, f)\n",
    "        print(f\"‚úì Saved result to cache: {cache_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to save cache: {e}\")\n",
    "\n",
    "def run_modular_evaluation(gt_csv, pred_csv, out_csv, config_options):\n",
    "    \"\"\"Run evaluation using the new modular metrics architecture.\"\"\"\n",
    "    print(\"üöÄ Starting modular evaluation pipeline...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create cache key\n",
    "    cache_key = get_cache_key([gt_csv, pred_csv], config_options)\n",
    "    \n",
    "    # Try cache first\n",
    "    cached_result = load_cached_result(cache_key)\n",
    "    if cached_result is not None:\n",
    "        pred_df, summary = cached_result\n",
    "        print(f\"‚ö° Evaluation completed in {time.time() - start_time:.1f}s (from cache)\")\n",
    "        return pred_df, summary\n",
    "    \n",
    "    # Initialize evaluators based on configuration\n",
    "    evaluators = initialize_evaluators(config_options)\n",
    "    \n",
    "    # Load and align datasets\n",
    "    print(\"üìä Loading and aligning datasets...\")\n",
    "    gt = pd.read_csv(gt_csv).sort_values(by=[STUDY_ID_COL_NAME]).reset_index(drop=True)\n",
    "    pred = pd.read_csv(pred_csv).sort_values(by=[STUDY_ID_COL_NAME]).reset_index(drop=True)\n",
    "    \n",
    "    # Find shared study IDs\n",
    "    shared_ids = set(gt[STUDY_ID_COL_NAME]).intersection(set(pred[STUDY_ID_COL_NAME]))\n",
    "    print(f\"Found {len(shared_ids)} shared study IDs\")\n",
    "    \n",
    "    gt = gt[gt[STUDY_ID_COL_NAME].isin(shared_ids)].reset_index(drop=True)\n",
    "    pred = pred[pred[STUDY_ID_COL_NAME].isin(shared_ids)].reset_index(drop=True)\n",
    "    \n",
    "    # Initialize results dataframe\n",
    "    results_df = pred.copy()\n",
    "    summary = {'mean_metrics': {}}\n",
    "    \n",
    "    # Run each metric evaluation\n",
    "    for metric_name, evaluator in evaluators.items():\n",
    "        try:\n",
    "            print(f\"üîÑ Computing {metric_name} metrics...\")\n",
    "            metric_start = time.time()\n",
    "            \n",
    "            # Compute metric using modular interface\n",
    "            metric_results = evaluator.compute_metric(gt, pred)\n",
    "            \n",
    "            # Add results to main dataframe\n",
    "            for col_name, values in metric_results.items():\n",
    "                results_df[col_name] = values\n",
    "                summary['mean_metrics'][col_name] = float(np.nanmean(values))\n",
    "            \n",
    "            metric_time = time.time() - metric_start\n",
    "            print(f\"  ‚úÖ {metric_name} completed in {metric_time:.2f}s\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå {metric_name} failed: {str(e)}\")\n",
    "            # Add placeholder columns for failed metrics\n",
    "            if metric_name == 'rouge':\n",
    "                results_df['rouge_l'] = [0.0] * len(results_df)\n",
    "            elif metric_name == 'bleu':\n",
    "                results_df['bleu4_score'] = [0.0] * len(results_df)\n",
    "            elif metric_name == 'bertscore':\n",
    "                results_df['bertscore_f1'] = [0.0] * len(results_df)\n",
    "            elif metric_name == 'perplexity':\n",
    "                results_df['perplexity_generated'] = [100.0] * len(results_df)\n",
    "                results_df['perplexity_reference'] = [100.0] * len(results_df)\n",
    "            elif metric_name == 'composite':\n",
    "                results_df['radcliq_v0'] = [0.0] * len(results_df)\n",
    "                results_df['radcliq_v1'] = [0.0] * len(results_df)\n",
    "            elif metric_name == 'semantic':\n",
    "                results_df['semantic_similarity'] = [0.0] * len(results_df)\n",
    "    \n",
    "    # Save results\n",
    "    print(\"üíæ Saving results...\")\n",
    "    results_df.to_csv(out_csv, index=False)\n",
    "    \n",
    "    summary_path = out_csv + '.summary.json'\n",
    "    with open(summary_path, 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    # Cache results\n",
    "    save_cached_result(cache_key, results_df, summary)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"‚úÖ Modular evaluation completed in {elapsed_time:.1f}s\")\n",
    "    \n",
    "    return results_df, summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abfb9110",
   "metadata": {},
   "source": [
    "# CXR Report Evaluation - Modular Metrics Notebook\n",
    "\n",
    "This notebook provides an interactive interface for evaluating CXR report generation using the **new modular metrics architecture**.\n",
    "\n",
    "## ‚ú® New Modular Architecture Features\n",
    "\n",
    "### üéØ Available Metrics\n",
    "- **ROUGE-L**: Content coverage and paraphrasing (pure Python, fastest)\n",
    "- **BLEU-4**: Precision-focused n-gram matching (fast)  \n",
    "- **BERTScore**: Semantic similarity with transformers (moderate speed)\n",
    "- **Perplexity**: Text fluency with GPT models (GPU-accelerated)\n",
    "- **Composite RadCliQ**: Clinical quality assessment (v0, v1)\n",
    "- **Semantic Embedding**: Medical-specific similarity with CheXbert\n",
    "\n",
    "### üèóÔ∏è Modular Benefits\n",
    "- **Flexible**: Enable/disable individual metrics as needed\n",
    "- **Cacheable**: Intelligent caching for faster repeated evaluations\n",
    "- **GPU-Optimized**: Automatic GPU detection and acceleration\n",
    "- **Consistent Interface**: All metrics follow the same API pattern\n",
    "- **Error Resilient**: Failed metrics don't break the entire evaluation\n",
    "\n",
    "## üìã Prerequisites\n",
    "\n",
    "### 1. Install Dependencies\n",
    "```bash\n",
    "pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "### 2. Configure Paths (Optional)\n",
    "Update `config.py` if using:\n",
    "- CheXbert model path (for semantic embedding)\n",
    "- Custom model checkpoints\n",
    "- GPU vs CPU preferences\n",
    "\n",
    "### 3. GPU Support (Recommended)\n",
    "- For Perplexity metrics: PyTorch with CUDA\n",
    "- For BERTScore: GPU acceleration available\n",
    "- For Semantic Embedding: CheXbert model benefits from GPU\n",
    "\n",
    "## üöÄ Quick Start\n",
    "1. **Configure file paths** in the interface below\n",
    "2. **Select desired metrics** (lightweight metrics are always enabled)\n",
    "3. **Enable GPU metrics** if you have GPU support\n",
    "4. **Run evaluation** and explore results with built-in visualizations\n",
    "\n",
    "## üìä Output Files\n",
    "- **Results CSV**: Per-report metric scores\n",
    "- **Summary JSON**: Aggregated statistics and metadata\n",
    "- **Cache files**: For faster repeated runs\n",
    "\n",
    "---\n",
    "*Updated for modular architecture - September 2025*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799e7283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Modular Evaluation Interface\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# File path configuration\n",
    "gt_csv_widget = widgets.Text(\n",
    "    value='path/to/ground_truth.csv',\n",
    "    placeholder='Enter path to ground truth CSV',\n",
    "    description='GT CSV:',\n",
    "    style={'description_width': '120px'}\n",
    ")\n",
    "\n",
    "pred_csv_widget = widgets.Text(\n",
    "    value='path/to/predictions.csv', \n",
    "    placeholder='Enter path to predictions CSV',\n",
    "    description='Pred CSV:',\n",
    "    style={'description_width': '120px'}\n",
    ")\n",
    "\n",
    "out_csv_widget = widgets.Text(\n",
    "    value='results/modular_evaluation_results.csv',\n",
    "    placeholder='Enter output path for results',\n",
    "    description='Output CSV:',\n",
    "    style={'description_width': '120px'}\n",
    ")\n",
    "\n",
    "# Modular metric configuration\n",
    "enable_rouge_widget = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='‚úÖ ROUGE-L (Always enabled - lightweight)',\n",
    "    disabled=True,\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "enable_bleu_widget = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='‚úÖ BLEU-4 (Always enabled - lightweight)', \n",
    "    disabled=True,\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "enable_bertscore_widget = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='ü§ñ BERTScore (Transformer-based semantic similarity)',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "use_idf_widget = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='    ‚Ü≥ Use IDF weighting (slightly slower but better quality)',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "enable_perplexity_widget = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='üî• Perplexity (GPU-accelerated text fluency)',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "perplexity_model_widget = widgets.Dropdown(\n",
    "    options=['distilgpt2', 'gpt2'],\n",
    "    value='distilgpt2',\n",
    "    description='    ‚Ü≥ Model:',\n",
    "    style={'description_width': '80px'}\n",
    ")\n",
    "\n",
    "enable_composite_widget = widgets.Checkbox(\n",
    "    value=True,\n",
    "    description='üè• Composite RadCliQ (Clinical quality assessment)',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "enable_semantic_widget = widgets.Checkbox(\n",
    "    value=False,\n",
    "    description='üß† Semantic Embedding (CheXbert-based medical similarity)',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "# Visualization options\n",
    "available_viz_metrics = ['rouge_l', 'bleu4_score', 'bertscore_f1', 'perplexity_generated', \n",
    "                        'perplexity_reference', 'radcliq_v0', 'radcliq_v1', 'semantic_similarity']\n",
    "\n",
    "viz_metrics_widget = widgets.SelectMultiple(\n",
    "    options=available_viz_metrics,\n",
    "    value=['rouge_l', 'bleu4_score', 'bertscore_f1'],\n",
    "    description='Visualization metrics:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "sample_size_widget = widgets.IntSlider(\n",
    "    value=1000,\n",
    "    min=100,\n",
    "    max=5000,\n",
    "    step=100,\n",
    "    description='Sample size for plots:',\n",
    "    style={'description_width': '150px'}\n",
    ")\n",
    "\n",
    "# Output area\n",
    "output = widgets.Output()\n",
    "\n",
    "# Global variables for results\n",
    "evaluation_results = None\n",
    "evaluation_summary = None\n",
    "\n",
    "def run_modular_evaluation_click(b):\n",
    "    \"\"\"Handler for the modular evaluation button.\"\"\"\n",
    "    with output:\n",
    "        clear_output()\n",
    "        print(\"üîÑ Running modular evaluation...\")\n",
    "        \n",
    "        try:\n",
    "            # Prepare configuration options\n",
    "            config_options = {\n",
    "                'enable_bertscore': enable_bertscore_widget.value,\n",
    "                'use_idf': use_idf_widget.value,\n",
    "                'enable_perplexity': enable_perplexity_widget.value,\n",
    "                'perplexity_model': perplexity_model_widget.value,\n",
    "                'enable_composite': enable_composite_widget.value,\n",
    "                'enable_semantic': enable_semantic_widget.value\n",
    "            }\n",
    "            \n",
    "            global evaluation_results, evaluation_summary\n",
    "            evaluation_results, evaluation_summary = run_modular_evaluation(\n",
    "                gt_csv=gt_csv_widget.value,\n",
    "                pred_csv=pred_csv_widget.value,\n",
    "                out_csv=out_csv_widget.value,\n",
    "                config_options=config_options\n",
    "            )\n",
    "            \n",
    "            # Display summary metrics\n",
    "            print(\"\\nüìä Evaluation Results Summary:\")\n",
    "            print(\"-\" * 40)\n",
    "            if 'mean_metrics' in evaluation_summary:\n",
    "                for metric, value in evaluation_summary['mean_metrics'].items():\n",
    "                    if metric == 'perplexity_generated' or metric == 'perplexity_reference':\n",
    "                        print(f\"  {metric}: {value:.2f}\")\n",
    "                    else:\n",
    "                        print(f\"  {metric}: {value:.4f}\")\n",
    "            \n",
    "            print(f\"\\nüìã Total reports evaluated: {len(evaluation_results)}\")\n",
    "            print(f\"üìÑ Results saved to: {out_csv_widget.value}\")\n",
    "            print(\"\\n‚úÖ Modular evaluation complete! Use visualization buttons below.\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error during evaluation: {str(e)}\")\n",
    "            import traceback\n",
    "            print(traceback.format_exc())\n",
    "\n",
    "# Visualization functions (updated for modular metrics)\n",
    "def plot_distributions_click(b):\n",
    "    \"\"\"Plot metric distributions.\"\"\"\n",
    "    with output:\n",
    "        if evaluation_results is None:\n",
    "            print(\"‚ùå Please run evaluation first!\")\n",
    "            return\n",
    "        \n",
    "        selected_metrics = [m for m in viz_metrics_widget.value if m in evaluation_results.columns]\n",
    "        if not selected_metrics:\n",
    "            print(\"‚ùå No valid metrics selected for visualization!\")\n",
    "            return\n",
    "        \n",
    "        print(\"üìà Plotting metric distributions...\")\n",
    "        plot_metric_distributions(evaluation_results, selected_metrics)\n",
    "\n",
    "def plot_boxplots_click(b):\n",
    "    \"\"\"Create boxplots for metrics.\"\"\"\n",
    "    with output:\n",
    "        if evaluation_results is None:\n",
    "            print(\"‚ùå Please run evaluation first!\")\n",
    "            return\n",
    "            \n",
    "        selected_metrics = [m for m in viz_metrics_widget.value if m in evaluation_results.columns]\n",
    "        if not selected_metrics:\n",
    "            print(\"‚ùå No valid metrics selected for visualization!\")\n",
    "            return\n",
    "        \n",
    "        print(\"üì¶ Creating boxplots...\")\n",
    "        plot_metric_boxplots(evaluation_results, selected_metrics)\n",
    "\n",
    "def plot_correlations_click(b):\n",
    "    \"\"\"Plot correlation heatmap.\"\"\"\n",
    "    with output:\n",
    "        if evaluation_results is None:\n",
    "            print(\"‚ùå Please run evaluation first!\")\n",
    "            return\n",
    "            \n",
    "        selected_metrics = [m for m in viz_metrics_widget.value if m in evaluation_results.columns]\n",
    "        if len(selected_metrics) < 2:\n",
    "            print(\"‚ùå Need at least 2 metrics for correlation analysis!\")\n",
    "            return\n",
    "        \n",
    "        print(\"üîó Plotting correlation heatmap...\")\n",
    "        plot_correlation_heatmap(evaluation_results, selected_metrics)\n",
    "\n",
    "def show_top_bottom_click(b):\n",
    "    \"\"\"Show top and bottom performing reports.\"\"\"\n",
    "    with output:\n",
    "        if evaluation_results is None:\n",
    "            print(\"‚ùå Please run evaluation first!\")\n",
    "            return\n",
    "            \n",
    "        selected_metrics = [m for m in viz_metrics_widget.value if m in evaluation_results.columns]\n",
    "        if not selected_metrics:\n",
    "            print(\"‚ùå No valid metrics selected!\")\n",
    "            return\n",
    "        \n",
    "        for metric in selected_metrics[:3]:  # Limit to first 3 metrics\n",
    "            print(f\"\\nüìã Analysis for {metric}:\")\n",
    "            show_top_bottom_reports(evaluation_results, metric, k=3)\n",
    "\n",
    "# Visualization helper functions (simplified versions)\n",
    "def plot_metric_distributions(df, metrics, bins=30, figsize=(12, 8)):\n",
    "    \"\"\"Plot distributions for selected metrics.\"\"\"\n",
    "    n_metrics = len(metrics)\n",
    "    n_cols = min(3, n_metrics)\n",
    "    n_rows = (n_metrics + n_cols - 1) // n_cols\n",
    "    \n",
    "    fig, axes = plt.subplots(n_rows, n_cols, figsize=figsize)\n",
    "    if n_metrics == 1:\n",
    "        axes = [axes]\n",
    "    elif n_rows == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        row, col = i // n_cols, i % n_cols\n",
    "        ax = axes[row, col] if n_rows > 1 else axes[col]\n",
    "        \n",
    "        data = df[metric].dropna()\n",
    "        if len(data) > 0:\n",
    "            sns.histplot(data, kde=True, bins=bins, ax=ax, alpha=0.7)\n",
    "            ax.set_title(f'{metric}')\n",
    "            mean_val, std_val = data.mean(), data.std()\n",
    "            ax.text(0.02, 0.98, f'Œº={mean_val:.3f}\\nœÉ={std_val:.3f}', \n",
    "                   transform=ax.transAxes, verticalalignment='top',\n",
    "                   bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "    \n",
    "    # Hide empty subplots\n",
    "    for i in range(n_metrics, n_rows * n_cols):\n",
    "        row, col = i // n_cols, i % n_cols\n",
    "        ax = axes[row, col] if n_rows > 1 else axes[col]\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_metric_boxplots(df, metrics, figsize=(12, 6)):\n",
    "    \"\"\"Create boxplots for metrics.\"\"\"\n",
    "    data = df[metrics].melt(var_name='metric', value_name='value')\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    ax = sns.boxplot(x='metric', y='value', data=data)\n",
    "    \n",
    "    means = data.groupby('metric')['value'].mean()\n",
    "    for i, metric in enumerate(metrics):\n",
    "        if metric in means.index:\n",
    "            ax.scatter(i, means[metric], marker='D', s=50, color='red', zorder=10)\n",
    "    \n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.title('Metric Distribution Comparison')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_correlation_heatmap(df, metrics, figsize=(10, 8)):\n",
    "    \"\"\"Plot correlation heatmap.\"\"\"\n",
    "    corr_matrix = df[metrics].corr()\n",
    "    \n",
    "    plt.figure(figsize=figsize)\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "    sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='coolwarm', \n",
    "                center=0, square=True, fmt='.3f')\n",
    "    plt.title('Metric Correlation Heatmap')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def show_top_bottom_reports(df, metric, k=5):\n",
    "    \"\"\"Show top and bottom reports by metric.\"\"\"\n",
    "    if metric not in df.columns:\n",
    "        print(f'Metric {metric} not found!')\n",
    "        return\n",
    "    \n",
    "    print(f\"üìä {metric}: {df[metric].min():.3f} to {df[metric].max():.3f} (Œº={df[metric].mean():.3f})\")\n",
    "    \n",
    "    # Top performers\n",
    "    top_df = df.nlargest(k, metric)\n",
    "    print(f\"üèÜ Top {k} reports:\")\n",
    "    display(top_df[[STUDY_ID_COL_NAME, REPORT_COL_NAME, metric]].round(3))\n",
    "    \n",
    "    # Bottom performers\n",
    "    bottom_df = df.nsmallest(k, metric)\n",
    "    print(f\"‚ö†Ô∏è Bottom {k} reports:\")\n",
    "    display(bottom_df[[STUDY_ID_COL_NAME, REPORT_COL_NAME, metric]].round(3))\n",
    "\n",
    "# Create interface buttons\n",
    "run_button = widgets.Button(\n",
    "    description='üöÄ Run Modular Evaluation',\n",
    "    button_style='success',\n",
    "    layout=widgets.Layout(width='220px', height='40px')\n",
    ")\n",
    "run_button.on_click(run_modular_evaluation_click)\n",
    "\n",
    "# Visualization buttons\n",
    "viz_buttons = [\n",
    "    ('üìà Distributions', plot_distributions_click),\n",
    "    ('üì¶ Boxplots', plot_boxplots_click),\n",
    "    ('üîó Correlations', plot_correlations_click),\n",
    "    ('üìã Top/Bottom', show_top_bottom_click)\n",
    "]\n",
    "\n",
    "viz_button_widgets = []\n",
    "for desc, handler in viz_buttons:\n",
    "    btn = widgets.Button(\n",
    "        description=desc,\n",
    "        button_style='info',\n",
    "        layout=widgets.Layout(width='140px', height='35px')\n",
    "    )\n",
    "    btn.on_click(handler)\n",
    "    viz_button_widgets.append(btn)\n",
    "\n",
    "clear_button = widgets.Button(\n",
    "    description='üóëÔ∏è Clear',\n",
    "    button_style='warning',\n",
    "    layout=widgets.Layout(width='80px', height='30px')\n",
    ")\n",
    "clear_button.on_click(lambda b: output.clear_output())\n",
    "\n",
    "# Display the interface\n",
    "print(\"üéõÔ∏è Modular CXR Report Evaluation Interface\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "print(\"\\nüìÅ File Configuration:\")\n",
    "display(widgets.VBox([gt_csv_widget, pred_csv_widget, out_csv_widget]))\n",
    "\n",
    "print(\"\\nüéØ Metric Configuration:\")\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<b>Lightweight metrics (always enabled):</b>\"),\n",
    "    enable_rouge_widget,\n",
    "    enable_bleu_widget,\n",
    "    widgets.HTML(\"<br><b>Advanced metrics (configurable):</b>\"),\n",
    "    enable_bertscore_widget,\n",
    "    use_idf_widget,\n",
    "    enable_perplexity_widget,\n",
    "    perplexity_model_widget,\n",
    "    enable_composite_widget,\n",
    "    enable_semantic_widget\n",
    "]))\n",
    "\n",
    "print(\"\\nüöÄ Evaluation:\")\n",
    "display(run_button)\n",
    "\n",
    "print(\"\\nüìä Visualization:\")\n",
    "display(widgets.VBox([\n",
    "    widgets.HTML(\"<b>Select metrics for visualization:</b>\"),\n",
    "    viz_metrics_widget,\n",
    "    sample_size_widget,\n",
    "    widgets.HTML(\"<br><b>Visualization Actions:</b>\"),\n",
    "    widgets.HBox(viz_button_widgets + [clear_button])\n",
    "]))\n",
    "\n",
    "print(\"\\nüìã Output:\")\n",
    "display(output)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 55)\n",
    "print(\"üí° Usage Tips:\")\n",
    "print(\"‚Ä¢ GPU metrics (Perplexity, BERTScore) are much faster with CUDA\")\n",
    "print(\"‚Ä¢ Results are automatically cached for faster repeated runs\")  \n",
    "print(\"‚Ä¢ Enable only needed metrics to optimize evaluation time\")\n",
    "print(\"‚Ä¢ Check utility_scripts/ folder for additional evaluation tools\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
